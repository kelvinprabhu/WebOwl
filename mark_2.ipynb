{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df08f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q~!34567890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3feb848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18619, 8)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://magazine.sebastianraschka.com/p/understanding-multimodal-llms\"\n",
    "url2 = \"https://medium.com/user-experience-design-1/cracking-the-code-of-vibe-coding-124b9288e551\"\n",
    "data = scrape_website(url2)\n",
    "print((len(data[0]), len(data[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42818533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy model and stop words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Custom unwanted words/phrases typical in web-scraped UI content\n",
    "custom_unwanted_phrases = [\n",
    "    \"login\", \"sign up\", \"signup\", \"create account\", \"forgot password\", \n",
    "    \"click here\", \"read more\", \"terms of service\", \"privacy policy\",\n",
    "    \"subscribe\", \"register\", \"next page\", \"previous page\", \"contact us\",\n",
    "    \"about us\", \"home\", \"back to top\", \"follow us\", \"view more\"\n",
    "]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text content, removing web-scraped UI junk.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove unwanted phrases\n",
    "    for phrase in custom_unwanted_phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Lemmatization and stopword removal\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.text not in stop_words \n",
    "        and token.text not in string.punctuation\n",
    "        and token.lemma_ not in custom_unwanted_phrases\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "044388f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.to(device)\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    return text_features.cpu().numpy()\n",
    "\n",
    "def get_image_embedding(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "        return image_features.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_url}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "706314d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.07689589e-01, -1.29972696e-01, -1.50694892e-01,\n",
       "        -2.86011547e-01, -2.99172521e-01, -1.29091665e-01,\n",
       "        -1.23714887e-01, -1.31375062e+00, -1.60170764e-01,\n",
       "         1.09250106e-01, -1.23362169e-01,  3.32235098e-01,\n",
       "         1.28503039e-01, -1.25202835e-02,  2.03039885e-01,\n",
       "        -2.05394387e-01, -5.69737665e-02,  7.06199706e-02,\n",
       "        -3.75526935e-01, -1.10198423e-01, -3.41495872e-03,\n",
       "        -3.45017970e-01,  4.62136269e-02, -3.99029732e-01,\n",
       "        -1.64712101e-01,  1.66862607e-01,  1.19063042e-01,\n",
       "        -3.19007337e-01,  5.79217523e-02, -2.54918635e-03,\n",
       "        -2.61154056e-01, -2.09452629e-01,  2.71080792e-01,\n",
       "         1.62050739e-01,  2.22844034e-02,  1.71472698e-01,\n",
       "         8.56696516e-02, -1.40662462e-01,  1.06197596e-02,\n",
       "        -2.51558602e-01,  4.29157317e-02, -3.73860568e-01,\n",
       "         1.42899960e-01,  8.22819844e-02, -1.10874474e-02,\n",
       "         2.87431777e-01,  2.66495347e-01, -1.55539393e-01,\n",
       "         2.66740113e-01,  1.02855302e-01, -4.56015944e-01,\n",
       "        -2.25645959e-01, -1.02813303e-01, -7.45297968e-02,\n",
       "        -5.54306805e-02,  7.11062178e-02,  2.19215304e-01,\n",
       "        -2.99581558e-01, -1.76582128e-01,  1.64172232e-01,\n",
       "         2.93283194e-01, -3.60727012e-02, -2.81839073e-03,\n",
       "         2.48233184e-01,  4.18999791e-03,  2.31131464e-02,\n",
       "         1.21383801e-01,  3.76812741e-02,  1.26599580e-01,\n",
       "         3.08701023e-02,  1.49381369e-01, -1.48663402e-01,\n",
       "        -3.89710665e-02,  1.82509810e-01, -2.25200310e-01,\n",
       "         7.62175471e-02,  2.77004808e-01, -1.96224391e-01,\n",
       "         1.27520114e-02, -1.31187558e-01,  1.44426838e-01,\n",
       "         1.12549461e-01,  2.93108076e-02,  1.52581424e-01,\n",
       "        -3.32236379e-01, -9.19098109e-02,  6.64922446e-02,\n",
       "        -5.80346808e-02,  3.93621713e-01,  1.83448344e-02,\n",
       "         5.28701842e-02, -4.71830219e-02, -1.83356810e+00,\n",
       "         3.67529154e-01,  5.10557294e-02,  2.46738195e-02,\n",
       "        -1.29849583e-01,  1.05696917e-03, -1.07761472e-01,\n",
       "        -1.60751417e-01,  1.79960400e-01, -1.80826969e-02,\n",
       "        -1.28903762e-01,  2.46667862e-03, -2.30764717e-01,\n",
       "         5.44733405e-02, -1.21957034e-01, -1.10994808e-01,\n",
       "         1.89204335e-01, -4.74114418e-02,  1.41745955e-01,\n",
       "         6.99259400e-01, -2.17066780e-02, -2.27901369e-01,\n",
       "        -1.67854175e-01,  1.59972310e-01,  1.61192983e-01,\n",
       "        -5.39598942e-01, -2.81761914e-01,  2.76811808e-01,\n",
       "        -1.88690349e-01, -3.16023469e-01, -4.68261614e-02,\n",
       "        -1.15577862e-01,  2.21457377e-01, -3.97606492e-02,\n",
       "         7.18869269e-02,  6.39059469e-02,  1.19988240e-01,\n",
       "         2.10717618e-01, -2.89190471e-01, -2.61860102e-01,\n",
       "        -1.13821402e-02,  7.38808346e+00,  1.09295323e-01,\n",
       "        -1.06014729e-01, -9.45405439e-02,  8.38694274e-02,\n",
       "         7.74286687e-04, -4.53150272e-01, -1.13213994e-01,\n",
       "        -8.88576284e-02,  3.48730385e-02, -7.52029866e-02,\n",
       "        -1.35600567e-01, -1.51882887e-01, -1.41117662e-01,\n",
       "         2.14355469e-01, -1.20288260e-01,  1.51909202e-01,\n",
       "        -6.15240708e-02, -8.52307677e-03,  3.50102544e-01,\n",
       "        -7.54973292e-03,  2.22319275e-01, -8.68901089e-02,\n",
       "        -3.83561403e-02, -1.69880748e-01, -7.03697801e-02,\n",
       "         3.76803935e-01, -2.99961567e-02, -2.93942541e-01,\n",
       "        -1.59491226e-02, -1.55417532e-01,  2.36521155e-01,\n",
       "        -1.48202941e-01, -5.90549782e-02, -4.66810107e-01,\n",
       "        -2.74312168e-01,  1.04771376e-01,  7.68869370e-02,\n",
       "        -1.13551766e-02, -2.09456578e-01,  1.45510197e-01,\n",
       "         1.47068352e-01, -8.47602487e-02,  1.03182696e-01,\n",
       "         4.57621753e-01,  1.14106342e-01,  2.29757786e-01,\n",
       "        -7.25882649e-02,  6.15279377e-03, -2.11604267e-01,\n",
       "         2.86503434e-01, -1.06992610e-01,  1.20958313e-01,\n",
       "         1.04159400e-01, -9.45739895e-02, -3.98914158e-01,\n",
       "        -3.20150793e-01,  1.13475516e-01, -2.02280223e-01,\n",
       "        -5.20849004e-02,  9.07275975e-02,  2.03710556e-01,\n",
       "        -3.56208503e-01,  4.34471890e-02, -5.61709404e-02,\n",
       "        -3.92912865e-01, -8.95835757e-02, -5.24749681e-02,\n",
       "        -1.00515671e-01, -2.76244581e-01,  4.12568152e-01,\n",
       "        -1.63892359e-02, -3.04627061e-01,  5.53058684e-02,\n",
       "        -1.00362241e-01,  1.28476471e-02, -2.42988765e-01,\n",
       "         6.51398450e-02,  2.32954696e-01,  5.38581051e-02,\n",
       "         9.77742225e-02, -3.64981115e-01,  2.42488757e-02,\n",
       "        -1.91690326e-01,  6.99060410e-03, -1.20076552e-01,\n",
       "         1.35402605e-02,  1.26990035e-01, -1.53893113e-01,\n",
       "         2.03017332e-02,  1.24153212e-01, -1.03629597e-01,\n",
       "        -9.15902033e-02,  1.08438820e-01, -3.82460728e-02,\n",
       "        -4.22737956e-01, -2.38925070e-02,  2.06774935e-01,\n",
       "         1.43032402e-01, -1.35897845e-01,  1.89853579e-01,\n",
       "         1.68916434e-01, -2.05125779e-01, -2.97665179e-01,\n",
       "         1.95843413e-01, -5.20706415e-01,  3.74879614e-02,\n",
       "        -1.77306235e-01,  1.33610651e-01, -7.01274723e-03,\n",
       "         1.19249858e-02, -2.88291276e-01, -2.53929049e-02,\n",
       "        -4.18358669e-02,  1.42215475e-01, -5.05501479e-02,\n",
       "        -2.42176950e-02, -3.28409635e-02, -1.70844644e-02,\n",
       "         1.23782344e-01,  6.65088594e-02,  3.09987009e-01,\n",
       "         9.60910395e-02,  4.62793857e-02, -1.21679455e-01,\n",
       "        -1.33253276e-01,  9.46982503e-02, -8.27706307e-02,\n",
       "        -1.01503998e-01, -2.19010219e-01,  1.36407867e-01,\n",
       "         4.20547128e-02,  6.33699819e-02,  3.64287496e-01,\n",
       "         4.36903127e-02, -3.75613332e-01,  1.74938932e-01,\n",
       "        -1.76479161e-01,  3.03630978e-01, -4.00666296e-02,\n",
       "        -1.43044710e-01,  1.68677717e-02,  1.52658939e-01,\n",
       "         3.63191813e-02, -2.54186243e-02, -6.05399162e-03,\n",
       "         1.27389550e-01,  2.21621826e-01,  9.91023704e-02,\n",
       "        -2.93874979e-01,  2.11247712e-01, -1.07020855e-01,\n",
       "         2.72470117e-01, -2.08959505e-01, -2.09879994e-01,\n",
       "         4.69986260e-01,  2.67421454e-02,  6.45211041e-02,\n",
       "        -7.34061524e-02, -1.62291795e-01, -4.50325347e-02,\n",
       "         1.15685686e-01, -9.71849188e-02, -1.12208053e-01,\n",
       "         1.92382634e-02,  2.20973685e-01,  3.14615697e-01,\n",
       "         4.58821654e-04, -1.19441748e-01,  1.33908987e-02,\n",
       "        -1.18417487e-01, -2.35194191e-01,  2.25345999e-01,\n",
       "        -2.17619389e-01, -3.32316101e-01,  1.99313700e-01,\n",
       "        -1.12637520e-01, -9.03068334e-02,  1.67981349e-02,\n",
       "         7.38204670e+00, -2.55631328e-01, -4.19100046e-01,\n",
       "         6.48577213e-02,  1.73452139e-01, -3.15469563e-01,\n",
       "         3.09132993e-01,  1.83579922e-02,  4.36671406e-01,\n",
       "         3.65249157e-01, -1.34953588e-01,  1.54117137e-01,\n",
       "        -7.35453218e-02,  2.86043406e-01,  2.90157586e-01,\n",
       "        -1.71123803e-01,  2.31903970e-01, -3.35506916e+00,\n",
       "        -2.10448831e-01,  2.57489949e-01, -8.17088559e-02,\n",
       "        -1.29776761e-01, -2.29724362e-01, -5.81093356e-02,\n",
       "        -5.27524948e-03,  3.26166570e-01,  3.62091035e-01,\n",
       "        -5.29640950e-02, -2.30816156e-02, -3.11308414e-01,\n",
       "        -3.35063726e-01, -1.48860753e-01,  3.59532982e-02,\n",
       "        -6.65697083e-03,  1.38171732e-01,  8.82174075e-03,\n",
       "         3.88475895e-01,  1.66688934e-01,  1.75511241e-02,\n",
       "        -6.73180073e-02, -6.21833950e-02,  4.43053365e-01,\n",
       "         9.88682359e-02,  3.09065640e-01, -2.55283155e-02,\n",
       "        -6.79346174e-02, -4.95070815e-02, -2.03073561e-01,\n",
       "        -1.18033879e-01,  1.23377055e-01,  4.48183626e-01,\n",
       "         4.12770331e-01, -2.71520525e-01,  4.04739439e-01,\n",
       "        -8.74136686e-02,  3.36598843e-01,  3.07126135e-01,\n",
       "        -1.94746107e-01, -2.27310389e-01,  1.09653801e-01,\n",
       "        -2.66441405e-01,  1.17060497e-01, -5.18154144e-01,\n",
       "         6.17194027e-02, -1.77172095e-01,  1.58027887e-01,\n",
       "        -5.11800945e-01, -1.63473375e-02, -3.39225501e-01,\n",
       "        -9.95405167e-02, -4.79199067e-02,  1.48304448e-01,\n",
       "         4.16693687e-02, -1.39583230e-01, -2.81810284e-01,\n",
       "         9.06821489e-02,  6.32326528e-02, -1.32830516e-01,\n",
       "         2.97115326e-01, -5.14471084e-02,  2.29210854e-02,\n",
       "        -5.62689751e-02, -4.69961315e-02,  7.66355917e-02,\n",
       "        -5.47631681e-02, -3.76300186e-01, -1.93400040e-01,\n",
       "         8.08137581e-02, -3.07341278e-01,  3.79954755e-01,\n",
       "        -1.17788121e-01, -1.33974090e-01,  1.73224196e-01,\n",
       "         2.78996885e-01,  2.36070588e-01,  1.08724654e-01,\n",
       "         4.90239799e-01,  1.93052769e-01,  1.05145425e-02,\n",
       "         3.38037089e-02,  1.08109221e-01,  2.60167003e-01,\n",
       "         2.40107298e-01,  6.85804188e-02,  1.81964144e-01,\n",
       "         1.16374716e-02, -4.36726362e-02,  2.96058357e-01,\n",
       "        -1.10134721e-01, -2.45226949e-01, -7.52515793e-02,\n",
       "        -1.40488535e-01, -2.40575582e-01,  1.70059562e-01,\n",
       "         1.63437769e-01, -2.81191729e-02, -1.76523656e-01,\n",
       "         6.72662333e-02,  1.36393696e-01, -3.90808731e-02,\n",
       "        -1.88044786e-01, -3.38925242e-01, -9.09900814e-02,\n",
       "         1.41524926e-01, -1.11137994e-01, -1.33867770e-01,\n",
       "        -3.90918255e-02, -1.10335052e-01, -2.28099093e-01,\n",
       "        -1.99908018e-02, -1.28536656e-01, -1.05066925e-01,\n",
       "        -4.67791975e-01,  2.65548706e-01,  1.34126544e-01,\n",
       "        -3.17738295e-01,  9.80640203e-03, -3.95267278e-01,\n",
       "        -9.30294991e-02,  1.40840024e-01, -4.29000147e-02,\n",
       "         1.77923948e-01,  1.32861823e-01,  8.94354731e-02,\n",
       "         1.57459229e-01, -3.55660096e-02, -1.39084503e-01,\n",
       "        -2.54436940e-01,  2.24331319e-02, -3.03474963e-02,\n",
       "        -2.29549676e-01,  1.55644968e-01, -1.74381793e-01,\n",
       "        -6.39417022e-03,  1.26784250e-01, -4.21562716e-02,\n",
       "        -1.96576282e-01,  1.63648099e-01,  9.34164673e-02,\n",
       "        -5.21449968e-02,  1.67624444e-01, -1.72859818e-01,\n",
       "        -1.18872680e-01,  1.38299987e-02,  5.31397685e-02,\n",
       "        -2.55348347e-03, -2.86474168e-01,  4.74867597e-03,\n",
       "         1.05646536e-01, -4.98751223e-01, -6.69244379e-02,\n",
       "         2.24120736e-01, -2.13291258e-01, -1.10521972e-01,\n",
       "         8.90306458e-02,  8.12539607e-02,  3.49353731e-01,\n",
       "        -2.09417790e-01,  1.08570695e-01, -4.89884496e-01,\n",
       "        -9.96946394e-02,  1.23162158e-01,  1.87032521e-02,\n",
       "        -2.34954700e-01,  5.26297867e-01, -1.02682181e-01,\n",
       "         3.23372692e-01,  3.25605631e-01, -2.16160938e-01,\n",
       "         6.21934161e-02, -1.29632831e-01,  1.89235419e-01,\n",
       "         4.08370376e-01, -6.01105094e-02, -8.90078321e-02,\n",
       "         7.78160989e-04, -1.80412978e-01, -1.74042344e-01,\n",
       "        -3.35502982e-01,  5.22074103e-01]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_embedding(\"This is a test sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2950b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: module 'chromadb' has no attribute '__version__'\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import numpy as np\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create embedding function (using same CLIP dimensions)\n",
    "clip_embedding_function = embedding_functions.DefaultEmbeddingFunction()\n",
    "\n",
    "def initialize_collection(collection_name=\"multimodal_rag\"):\n",
    "    \"\"\"\n",
    "    Initialize ChromaDB collection with multimodal support\n",
    "    \"\"\"\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=clip_embedding_function,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # Similarity metric\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "def store_data(collection_name, text, text_embedding, image_url=None, image_embedding=None):\n",
    "    \"\"\"\n",
    "    Store data in ChromaDB with multimodal support\n",
    "    \"\"\"\n",
    "    collection = chroma_client.get_collection(collection_name)\n",
    "    \n",
    "    # Create document ID\n",
    "    doc_id = str(hash(text + (image_url if image_url else \"\")))\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        \"image_url\": image_url if image_url else \"\",\n",
    "        \"source_type\": \"multimodal\" if image_url else \"text_only\",\n",
    "        \"processed_text\": preprocess_text(text)\n",
    "    }\n",
    "    \n",
    "    # Convert embeddings to list format\n",
    "    text_embedding = text_embedding[0].tolist() if isinstance(text_embedding, np.ndarray) else text_embedding\n",
    "    image_embedding = image_embedding[0].tolist() if image_embedding and isinstance(image_embedding, np.ndarray) else None\n",
    "    \n",
    "    # Store with both embeddings in metadata\n",
    "    collection.add(\n",
    "        ids=[doc_id],\n",
    "        documents=[text],\n",
    "        embeddings=[text_embedding],\n",
    "        metadatas=[{\n",
    "            **metadata,\n",
    "            \"image_embedding\": image_embedding if image_embedding else None\n",
    "        }]\n",
    "    )\n",
    "\n",
    "def retrieve_relevant_chunks(query, collection_name=\"multimodal_rag\", top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve relevant chunks from ChromaDB\n",
    "    \"\"\"\n",
    "    collection = chroma_client.get_collection(collection_name)\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = get_text_embedding(query)[0].tolist()\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Format results similar to Qdrant's output\n",
    "    class Result:\n",
    "        def __init__(self, payload, score):\n",
    "            self.payload = payload\n",
    "            self.score = 1 - score  # Convert distance to similarity\n",
    "    \n",
    "    formatted_results = []\n",
    "    for doc, meta, dist in zip(results['documents'][0], \n",
    "                              results['metadatas'][0], \n",
    "                              results['distances'][0]):\n",
    "        formatted_results.append(\n",
    "            Result(payload={\n",
    "                \"text\": doc,\n",
    "                \"image_url\": meta.get(\"image_url\"),\n",
    "                \"processed_text\": meta.get(\"processed_text\"),\n",
    "                \"metadata\": meta\n",
    "            }, score=1 - dist)\n",
    "        )\n",
    "    \n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "collection = initialize_collection()\n",
    "\n",
    "# Store data (example)\n",
    "text = \"Example content about AI\"\n",
    "text_embedding = get_text_embedding(text)\n",
    "image_url = \"http://example.com/image.jpg\"\n",
    "image_embedding = get_image_embedding(image_url)\n",
    "\n",
    "store_data(\"multimodal_rag\", text, text_embedding, image_url, image_embedding)\n",
    "\n",
    "# Retrieve\n",
    "results = retrieve_relevant_chunks(\"What is AI?\", \"multimodal_rag\")\n",
    "for result in results:\n",
    "    print(f\"Score: {result.score:.3f}\")\n",
    "    print(f\"Text: {result.payload['text'][:100]}...\")\n",
    "    if result.payload['image_url']:\n",
    "        print(f\"Image: {result.payload['image_url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51e0843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Ollama client\n",
    "ollama_client = Client(host='http://localhost:11434')\n",
    "\n",
    "def retrieve_relevant_chunks(query, collection_name=\"multimodal_rag\", top_k=3):\n",
    "    \"\"\"Enhanced retrieval with hybrid search (text + image)\"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = get_text_embedding(query)\n",
    "    \n",
    "    # Search in Qdrant with hybrid approach\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=(\"text\", query_embedding[0].tolist()),\n",
    "        query_filter=None,  # Add filters here if needed\n",
    "        limit=top_k,\n",
    "        with_vectors=True,\n",
    "        with_payload=True,\n",
    "        score_threshold=0.3  # Minimum similarity score\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_response(query, retrieved_chunks, use_images=False):\n",
    "    \"\"\"\n",
    "    Generate response using deepseek-r1:14b with enhanced prompting\n",
    "    Args:\n",
    "        use_images: Whether to include image context in the prompt\n",
    "    \"\"\"\n",
    "    # Prepare text context\n",
    "    text_context = \"\\n\".join([\n",
    "        f\"Source {i+1}:\\n{chunk.payload['text']}\\n\" \n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ])\n",
    "    \n",
    "    # Prepare image context if available and requested\n",
    "    image_context = \"\"\n",
    "    if use_images:\n",
    "        image_descriptions = []\n",
    "        for chunk in retrieved_chunks:\n",
    "            if chunk.payload.get('image_url'):\n",
    "                img_desc = f\"Image from {chunk.payload['image_url']} related to: {chunk.payload['text'][:100]}...\"\n",
    "                image_descriptions.append(img_desc)\n",
    "        if image_descriptions:\n",
    "            image_context = \"\\nVisual Context:\\n\" + \"\\n\".join(image_descriptions) + \"\\n\"\n",
    "    \n",
    "    # Craft optimized prompt for deepseek-r1:14b\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are an AI assistant that answers questions using the provided context. \n",
    "Be concise yet informative. If unsure, say you don't know.<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Context Information:\n",
    "{text_context}\n",
    "{image_context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a detailed answer based on the context. If the context doesn't contain the answer, \n",
    "say \"I couldn't find that information in my sources.\"<|im_end|>\n",
    "\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate response with optimized parameters for deepseek-r1\n",
    "    response = ollama_client.generate(\n",
    "        model='deepseek-r1:14b',\n",
    "        prompt=prompt,\n",
    "        options={\n",
    "            'temperature': 0.3,  # Lower for more factual responses\n",
    "            'top_p': 0.9,\n",
    "            'num_ctx': 4096,     # Utilize full context window\n",
    "            'repeat_penalty': 1.1\n",
    "        },\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    return response['response'].strip()\n",
    "\n",
    "def multimodal_qa(query, collection_name=\"multimodal_rag\", top_k=3):\n",
    "    \"\"\"End-to-end QA pipeline with multimodal capabilities\"\"\"\n",
    "    # Retrieve relevant chunks\n",
    "    chunks = retrieve_relevant_chunks(query, collection_name, top_k)\n",
    "    \n",
    "    # First try with text only\n",
    "    answer = generate_response(query, chunks, use_images=False)\n",
    "    \n",
    "    # If answer is uncertain, try including image context\n",
    "    if \"I couldn't find\" in answer or \"don't know\" in answer.lower():\n",
    "        answer = generate_response(query, chunks, use_images=True)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9750a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_website(url, collection_name=\"multimodal_rag\", max_images=20):\n",
    "    \"\"\"\n",
    "    Enhanced website processing with better error handling and metadata capture\n",
    "    Args:\n",
    "        url: Website URL to process\n",
    "        collection_name: VectorDB collection name\n",
    "        max_images: Maximum number of images to process per page\n",
    "    Returns:\n",
    "        Tuple (success: bool, num_images_processed: int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Scrape website with enhanced metadata\n",
    "        text_content, image_urls = scrape_website(url)\n",
    "        \n",
    "        if not text_content:\n",
    "            print(f\"No text content found at {url}\")\n",
    "            return False, 0\n",
    "        \n",
    "        # Process text with additional metadata\n",
    "        processed_text = preprocess_text(text_content)\n",
    "        text_embedding = get_text_embedding(processed_text)\n",
    "        \n",
    "        # Process images with better error handling\n",
    "        image_embeddings = []\n",
    "        successful_images = 0\n",
    "        for img_url in image_urls[:max_images]:\n",
    "            try:\n",
    "                img_embedding = get_image_embedding(img_url)\n",
    "                if img_embedding is not None:\n",
    "                    image_embeddings.append(img_embedding)\n",
    "                    successful_images += 1\n",
    "            except Exception as img_e:\n",
    "                print(f\"Failed to process image {img_url}: {str(img_e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Prepare metadata payload\n",
    "        metadata = {\n",
    "            \"source_url\": url,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"text_length\": len(text_content),\n",
    "            \"num_images\": successful_images,\n",
    "            \"processed_text_length\": len(processed_text)\n",
    "        }\n",
    "        \n",
    "        # Store in vector DB - now supports multiple images\n",
    "        for i, (img_url, img_embedding) in enumerate(zip(image_urls[:max_images], image_embeddings)):\n",
    "            store_data(\n",
    "                collection_name=collection_name,\n",
    "                text=text_content,\n",
    "                text_embedding=text_embedding,\n",
    "                image_url=img_url,\n",
    "                image_embedding=img_embedding,\n",
    "                metadata={**metadata, \"image_index\": i}\n",
    "            )\n",
    "        \n",
    "        # Also store a text-only version if we have images\n",
    "        if successful_images > 0:\n",
    "            store_data(\n",
    "                collection_name=collection_name,\n",
    "                text=text_content,\n",
    "                text_embedding=text_embedding,\n",
    "                image_url=None,\n",
    "                image_embedding=None,\n",
    "                metadata=metadata\n",
    "            )\n",
    "        \n",
    "        return True, successful_images\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing website {url}: {str(e)}\")\n",
    "        return False, 0\n",
    "\n",
    "def ask_question(query, collection_name=\"multimodal_rag\", include_sources=False):\n",
    "    \"\"\"\n",
    "    Enhanced QA with multimodal support and source attribution\n",
    "    Args:\n",
    "        query: User question\n",
    "        collection_name: VectorDB collection name\n",
    "        include_sources: Whether to include source URLs in response\n",
    "    Returns:\n",
    "        Dict: {\n",
    "            \"answer\": str,\n",
    "            \"sources\": list[str] (if include_sources=True),\n",
    "            \"confidence\": float (0-1)\n",
    "        }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve relevant chunks with hybrid search\n",
    "        retrieved_chunks = retrieve_relevant_chunks(query, collection_name)\n",
    "        \n",
    "        if not retrieved_chunks:\n",
    "            return {\n",
    "                \"answer\": \"I couldn't find any relevant information to answer your question.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Generate initial text-based response\n",
    "        answer = generate_response(query, retrieved_chunks, use_images=False)\n",
    "        \n",
    "        # Calculate confidence score based on retrieval scores\n",
    "        confidence = min(1.0, max(0.0, sum(c.score for c in retrieved_chunks) / len(retrieved_chunks)))\n",
    "        \n",
    "        # If low confidence, try with image context\n",
    "        if confidence < 0.5:\n",
    "            image_answer = generate_response(query, retrieved_chunks, use_images=True)\n",
    "            if \"I couldn't find\" not in image_answer:\n",
    "                answer = image_answer\n",
    "                confidence = min(1.0, confidence + 0.2)  # Boost confidence\n",
    "        \n",
    "        # Prepare sources if requested\n",
    "        sources = []\n",
    "        if include_sources:\n",
    "            sources = list({c.payload.get('metadata', {}).get('source_url') \n",
    "                          for c in retrieved_chunks if c.payload.get('metadata', {}).get('source_url')})\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources if include_sources else None,\n",
    "            \"confidence\": round(confidence, 2)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error answering question: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"An error occurred while processing your question.\",\n",
    "            \"sources\": [],\n",
    "            \"confidence\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18fdfb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing website https://magazine.sebastianraschka.com/p/understanding-multimodal-llms: store_data() got an unexpected keyword argument 'metadata'\n",
      "Processed 0 images\n",
      "Error answering question: Collection multimodal_rag not found\n",
      "Answer: An error occurred while processing your question.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Error answering question: Collection multimodal_rag not found\n",
      "An error occurred while processing your question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3075/3303136241.py:13: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# Process a website\n",
    "success, num_images = process_website(url, max_images=8)\n",
    "print(f\"Processed {num_images} images\")\n",
    "\n",
    "# Ask a question with sources\n",
    "response = ask_question(\"What is this website about?\", include_sources=True)\n",
    "print(\"Answer:\", response[\"answer\"])\n",
    "print(\"Sources:\", response[\"sources\"])\n",
    "print(\"Confidence:\", response[\"confidence\"])\n",
    "\n",
    "# Simple query\n",
    "answer = ask_question(\"When was this content last updated?\")\n",
    "print(answer[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linux-deep-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
